# üöÄ Quick Start - ChatIAS Pro 2.0 com RAG + Ollama Embeddings

## Setup Completo em 5 Passos

### 1Ô∏è‚É£ Instale o Ollama

**Windows/Mac/Linux:**
```bash
# Instale: https://ollama.com/download

# Inicie o servi√ßo
ollama serve

# Teste
ollama list
```

### 2Ô∏è‚É£ Baixe o Modelo de Embeddings

```bash
# Modelo recomendado (768 dimens√µes, melhor qualidade)
ollama pull nomic-embed-text

# Ou alternativas:
# ollama pull mxbai-embed-large  # 1024 dims, mais lento
# ollama pull all-minilm         # 384 dims, mais r√°pido
```

### 3Ô∏è‚É£ Inicie o Qdrant (Docker)

```bash
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant

# Verifique
curl http://localhost:6333/healthz
```

### 4Ô∏è‚É£ Popule o Qdrant com Dados

```bash
# Gera embeddings REAIS com Ollama
node scripts/populate-qdrant.js
```

**Output esperado:**
```
üöÄ ChatIAS - Qdrant Population Script (WITH REAL EMBEDDINGS)
=============================================================

1Ô∏è‚É£  Checking Qdrant at http://localhost:6333...
   ‚úÖ Qdrant is running

2Ô∏è‚É£  Checking Ollama at http://localhost:11434...
   ‚úÖ Ollama is running
   ‚úÖ Model "nomic-embed-text" is available

3Ô∏è‚É£  Testing embedding generation...
   ‚úÖ Embedding generated: 768 dimensions

4Ô∏è‚É£  Checking collection "knowledge_base"...
   üì¶ Collection not found, creating with 768 dimensions...
   ‚úÖ Collection created

5Ô∏è‚É£  Generating embeddings for 10 documents...
   ‚è≥ This will take a few moments...

   üìù Processing document 1/10... ‚úÖ
   üìù Processing document 2/10... ‚úÖ
   ...

   ‚¨ÜÔ∏è  Uploading to Qdrant...
   ‚úÖ Upload complete

6Ô∏è‚É£  Collection info:
   üìä Points: 10
   üìä Vectors: 10
   üìä Dimensions: 768
   üìä Status: green

‚úÖ Done! Qdrant is ready with REAL embeddings!
```

### 5Ô∏è‚É£ Inicie o ChatIAS

```bash
node server-v2.js
```

**Output esperado:**
```
[mcp] Ollama connected (url: http://localhost:11434)
[embedder] Initializing Ollama embedder for RAG...
[embedder] Testing embedding generation...
[embedder] ‚úÖ Ollama embedder ready (model: nomic-embed-text, dimensions: 768)
[rag] Initializing RAG system...
[rag] ‚úÖ Collection "knowledge_base" ready
[system] ‚úÖ System Ready

Server running on http://localhost:4174
Chat UI: http://localhost:4174/chat-v2
```

## üéØ Testando o Sistema

### Teste 1: Conversa Simples (LLM-only)

```
Voc√™: oi
Bot: [Strategy: LLM ‚ö° 95%] Ol√°! Como posso ajudar?
```

**Logs:**
```
[decision] Strategy: llm (95% - Greeting detected)
[llm] Using LLM-only mode (fastest)
[response] Generated in 450ms
```

### Teste 2: Pergunta com Conhecimento (RAG)

```
Voc√™: o que √© o ChatIAS?
Bot: [Strategy: RAG üìö 90%] O ChatIAS Pro 2.0 √© um sistema inteligente...
```

**Logs:**
```
[decision] Strategy: rag (90% - Knowledge question detected)
[rag] Searching knowledge base...
[embedder] Generating embedding with Ollama...
[embedder] ‚úÖ Embedding generated (768 dims, 85ms)
[rag] ‚úÖ Found 3 relevant documents (45ms total)
[llm] Generating response with context...
[response] Generated in 1250ms (using RAG)
```

### Teste 3: Busca Direta na API

```bash
curl -X POST http://localhost:4174/api/rag/search \
  -H "Content-Type: application/json" \
  -d '{"query": "como funciona o RAG?"}'
```

**Response:**
```json
{
  "success": true,
  "results": [
    {
      "score": 0.89,
      "text": "RAG (Retrieval-Augmented Generation) √© uma t√©cnica..."
    }
  ],
  "duration": 120
}
```

## üìä Monitorando Embeddings

### Info do RAG

```bash
curl http://localhost:4174/api/rag/info
```

**Response:**
```json
{
  "enabled": true,
  "baseUrl": "http://localhost:6333",
  "collectionName": "knowledge_base",
  "dimensions": 768,
  "hasEmbedder": true,
  "embedderInfo": {
    "model": "nomic-embed-text",
    "dimensions": 768,
    "totalEmbeddings": 45,
    "cacheHitRate": "78.5%",
    "avgDuration": "82ms"
  }
}
```

## üîß Troubleshooting

### Ollama n√£o conecta

```bash
# Verifique se est√° rodando
ps aux | grep ollama

# Reinicie
killall ollama
ollama serve

# Teste
ollama list
```

### Modelo de embedding n√£o encontrado

```bash
# Liste modelos dispon√≠veis
ollama list

# Pull novamente
ollama pull nomic-embed-text

# Teste manualmente
ollama run nomic-embed-text "test"
```

### Embeddings lentos

**Problema:** Cada embedding demora > 2 segundos

**Solu√ß√£o:**
1. Use modelo menor: `ollama pull all-minilm` (384 dims, 3x mais r√°pido)
2. Aumente batch size no c√≥digo
3. Verifique CPU/RAM dispon√≠vel

### Qdrant retorna resultados ruins

**Problema:** RAG retorna documentos irrelevantes

**Solu√ß√µes:**
1. Ajuste `scoreThreshold` (padr√£o: 0.7):
   ```javascript
   ragScoreThreshold: 0.6  // Menos restritivo
   ```

2. Aumente `topK` (padr√£o: 5):
   ```javascript
   ragTopK: 10  // Mais resultados
   ```

3. Repovoar com dados mais relevantes
4. Usar modelo de embedding melhor: `mxbai-embed-large`

## üéì Customiza√ß√£o

### Adicionar Seus Documentos

Edite `scripts/populate-qdrant.js`:

```javascript
const documents = [
  {
    text: "Seu conhecimento aqui...",
    metadata: {
      source: "meu_doc",
      category: "categoria",
      tags: ["tag1", "tag2"]
    }
  },
  // ... mais documentos
];
```

Depois rode:
```bash
node scripts/populate-qdrant.js
```

### Usar Modelo Diferente

Em `.env`:
```bash
OLLAMA_EMBED_MODEL=mxbai-embed-large
```

**Aten√ß√£o:** Se mudar o modelo, precisa:
1. Deletar collection antiga
2. Recriar com novas dimens√µes
3. Repopular todos os documentos

## üìà Performance

| Modelo | Dimens√µes | Velocidade | Qualidade | Recomendado para |
|--------|-----------|-----------|-----------|------------------|
| `nomic-embed-text` | 768 | ~100ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Produ√ß√£o (balanceado) |
| `mxbai-embed-large` | 1024 | ~200ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√°xima qualidade |
| `all-minilm` | 384 | ~30ms | ‚≠ê‚≠ê‚≠ê | Desenvolvimento r√°pido |

**Benchmarks (i7-10700K):**
- Embedding generation: 50-150ms
- Qdrant search: 5-20ms
- Total RAG overhead: ~100-200ms vs LLM-only

## ‚úÖ Checklist Completo

- [ ] Ollama instalado e rodando
- [ ] Modelo `nomic-embed-text` baixado
- [ ] Qdrant rodando no Docker
- [ ] Script de popula√ß√£o executado com sucesso
- [ ] ChatIAS iniciado e conectado
- [ ] Teste: conversa simples funciona
- [ ] Teste: pergunta sobre conhecimento usa RAG
- [ ] Logs mostram estrat√©gias corretas
- [ ] `/api/rag/info` retorna dados v√°lidos

## üéâ Pronto!

Seu ChatIAS agora tem:
- ‚úÖ RAG funcional com embeddings reais
- ‚úÖ Busca sem√¢ntica precisa
- ‚úÖ Decis√£o inteligente autom√°tica
- ‚úÖ Cache de embeddings
- ‚úÖ Fallback graceful

**Pr√≥ximos passos:**
1. Adicione seus documentos reais
2. Ajuste thresholds para seu caso de uso
3. Monitore performance e cache hit rate
4. Expanda a base de conhecimento gradualmente
